# seckill3
# 秒杀系统笔记

<img src="img/image-20211116182433366.png" alt="image-20211116182433366" style="zoom: 80%;" />

<img src="img/image-20211116182500271.png" alt="image-20211116182500271" style="zoom:80%;" />

![image-20211116182606052](img/image-20211116182606052.png)

# 0. 项目准备

### 整体架构

![image-20211115204326003](img/image-20211115204326003.png)

- Spring解决的是整个后端系统的对象管理问题，目的是解耦，可插拔

- 利用MyBatis解决数据层

- 利用Spring MVC 解决视图层,Controller接收浏览器的请求，再传给业务层实现，如果需要与数据库交互，再传给数据层实现；得到结果后Controller将请求的内容通过Model对象封装传给ViewResolver；再利用ViewResolver将数据传给浏览器。（并不是说Spring MVC M解决数据层，V解决视图层，C解决业务层，这是个误区），Spring MVC的三个组件分别是Controller控制组件，Model数据组件，ViewResolver试图组件



### 环境搭建

- 在服务器上装好MySQL，把准备好的表上传上去，本地用Nvicat连接服务器数据库
- 本地安装开发工具 IDEA JDK Maven VSCode（编译前端代码）
- IDEA导入后台代码，修改配置文件，运行
- Vscode导入前端代码，运行



![image-20211116182633572](img/image-20211116182633572.png)



### Spring + Spring MVC + Spring Boot

<img src="img/image-20211116183902992.png" alt="image-20211116183902992" style="zoom: 50%;" />

### spring

**IoC和AOP依赖ApplicationContext和BeanFactory实现，他们俩都是bean工厂**

- **BeanFactory**是spring的原始接口，是一个比较底层的API，针对原始结构的实现类功能比较单一，主要是spring内部人士使用，BeanFactory接口实现的容器，**特点是在每次获取对象时才会创建对象。**

- **ApplicationContext**继承了BeanFactory接口，拥有BeanFactory的全部功能，并且扩展了很多高级特性，是开发者使用的，**每次容器启动时就会创建所有的对象。**

**入口文件**

<img src="img/image-20211116200334390.png" alt="image-20211116200334390" style="zoom: 67%;" />



==**spring利用Ioc（控制反转）管理对象**==，就是把对对象的控制交给Spring来做，对象可以自动装载，降低对象之间的耦合度，可插拔（想要替换某一个bean很方便），通常情况下，需要单例的用IoC来管理，因为这是可复用的，有很多实例化对象的（例如实体类）就不用IoC。

- **同时IoC可以管理bean的生命周期**（例如用`@PostConstruct`和`@PreDestory`在bean的初始化和销毁前做一些业务）。
- **也可以管理bean的作用域**，spring默认只会实例化一个对象，用`@Scope("prototype")`就变成每次都实例化一个bean

==**AOP面向切面编程**==

定义一个切面类，在该类里定义PointCut切点（也就是切入的位置）和Advice通知（在该切面完成的工作和使用时间），从而在相应的JointPoint连接点（即相应的那个方法）中生效。在代码中体现的非常简单，不需要修改任何源代码，写一个切面类就可以了。

<img src="img/image-20211116203624948.png" alt="image-20211116203624948" style="zoom:67%;" />



### Spring MVC

黄蓝红四个地方的组件是最重要的

<img src="img/image-20211116212648113.png" alt="image-20211116212648113" style="zoom: 33%;" />

​	①所有前端的请求被==**DispatcherServlet**==（DispatcherSevlet是Spring MVC最核心的组件，负责将请求分发接收），根据 ==**HandlerMapping**== 映射到 ==**Handler**==处理器——⽣成 Handler 和 ==**HandlerInterceptor**==（处理器拦截器，是一个接口，实现该接口可以进行一些拦截处理，可以分别在调用HandlerAdapter之前、之后和视图响应数据之后三个地方进行拦截），Handler 和 HandlerInterceptor 以 ==**HandlerExecutionChain**==（处理器执⾏链，包括两部分内容： Handler 和HandlerInterceptor） 的形式⼀并返回给DispatcherServlet；

​	②DispatcherServlet 通过 **==HandlerAdpater==** （处理器适配器， Handler 执⾏业务⽅法之前，需要进⾏⼀系列的操作包括表单的数据验证、数据类型的转换、将表单数据封装到 POJO 等，这⼀些列操作都是由HandlerAdapter 完成）调⽤ Handler 的⽅法完成业务逻辑处理。 此时会返回⼀个 ==**ModelAndView**==（封装了模型数据和视图信息  ） 对象给 DispatcherServlet，**但是这个项目返回的都是json格式的数据，那么就直接由HandlerAdapter返回**（添加@ResponseBody注解）；

​	③随后DispatcherServlet 将获取的 ModelAndView 对象传给 ==**ViewResolver**==视图解析器，将逻辑视图解析成物理视图。  然后DispatcherServlet 通过 View 将模型数据填充到视图中，DispatcherServlet 将渲染之后的视图响应给客户端。  



### MyBatis

<img src="img/image-20211117173412375.png" alt="image-20211117173412375" style="zoom:80%;" />



代码：

公共部分：

- ==ErrorCode==：定义了一些异常编号。
- ==BusinessException==： 有两个成员变量①编号（在ErrorCode里定义了）②描述信息。当项目遇到异常的时候，全部包装成这个异常往外抛，**这个异常继承了`RuntimeException`(可以try-catch也可以不try-catch)，因为项目是分层的，一层一层的调用，所以无论在哪一层抛异常，都会被Controller感受到，所以在controller统一处理就可以了，不用很麻烦的在每一层都处理。** 在controller如何处理（在controller写了一个ExceptionAdvice类，加上@ControllerAdvice注解，在该类里定义一个方法，参数是异常(Exception e)，该方法加@ExceptionHandler（没有捕获的异常会以参数的形式传入加了@ExceptionHandler注解的那个方法中）和@ResponseBody表示在发生异常时调用该方法，返回异常的json格式数据）
- ==ResponseModel==：用来统一视图层返回给前端的json数据，例如{status：0，{id：1，title：“”}}（status==0表示业务状态成功，不加状态码前端就无法知道你返回的数据是否正确？数据格式是什么？）。
- ==Toolbox==：里面有一个md5加密方法，将用户的明文密码存成密文。
- 创建两个资源文件，分为开发时用、测试时用，在创建一个开关文件，选择使用哪个资源配置文件

# 1. 用户注册与登录

![image-20211117194201213](img/image-20211117194201213.png)

## 1.1 ==代码实现==

**定义实体类User**，每个成员变量加注解@NotNull(针对Integer类型)或@NotBlank(针对string类型)，定义一个ObjectValidator验证类，里面定义一个validate方法，该方法传入一个Object类型的对象，用Validator的validate方法验证是否为空，定义一个map，如果有问题就把错误信息装入map。

**dao层**：

- 写一个**UserMapper接口**，定义需要用到的各种方法，
- 再在**UserMapper.xml**写SQL语句

**service层**：

- 写一个**UserService接口**，定义三个方法`register(User user)`,`login(String phone, String password)`,`findUserById(int id)`

- **UserServiceImpl**继承该接口，注入userMapper和validator，
  - 定义**register方法**，先判断传入的user是否有空值，然后再插入到数据库
  - 定义**login方法**，传入手机号和密码(密码是加密后的密码)，如果查不到返回错误信息，查得到就返回user（表示验证通过）

**controller层：** 

- 访问路径`/user`,注入userService
  - **getOTP方法**（GET请求）将传入的手机号与生成的随机验证码绑定到session里，返回`new ResponseModel()`(ResponseModel()默认是成功的)
  - rigister方法（POST请求）先验证OTP，如果没有问题，把密码加密一下，再调注册方法，返回`new ResponseModel()`代表成功
  - **login方法**（POST请求）先判断是否为空，然后调登录方法，把得到的user信息存入session中，返回`new ResponseModel()`代表成功
  - logout方法（GET请求），注销session就可以了
  - **getUser方法**（GET请求），右上角显示登录信息查登录状态的方法，通过session查询user信息



## 1.2 ==状态管理==

首先，跨域问题不适合用AOP来做，因为是否登录的某些功能是根据url进行的过滤，而不是根据某个方法进行的过滤（AOP主要是面向某个方法或者某个对象进行的过滤），适合用MVC的拦截器处理（拦截器面向url进行的过滤），因为一个拦截器可以拦截多个controller

写一个`LoginCheckInterceptor`拦截器，重写preHandle方法（还有两个方法分别表示返回ModelAndView给 DispatcherServlet的时候，和操作结束后返回ModelAndView给ViewResolver的时候）表示在操作之前过滤，检查登录状态，再写一个配置类实现WebMvcConfigurer接口，注册该拦截器并配置拦截器的拦截url（在本项目中只拦截下单这一个路径 ）



## 1.3 ==**跨域问题**==

​		跨域（CORS）是指不同域名之间相互访问。跨域，指的是浏览器不能执行其他网站的脚本，它是由浏览器的同源策略所造成的，是浏览器对于JavaScript所定义的安全限制策略。

### **什么情况会跨域**

- 同一协议， 如http或https
- 同一IP地址, 如127.0.0.1
- 同一端口, 如8080

以上三个条件中有一个条件不同就会产生跨域问题。

### 解决方法

1. **《规避法》**利用Nginx做代理，规避跨域问题，所有请求都访问Nginx，由Nginx分配给相应的地址。以`/static`开头的请求都交给前端服务器，以`/other`开头的请求交给后端服务器，实现代理即可。弊端是前端和后端的url要有严格的规范，要不然Nginx不好做判断，缺乏了一些灵活性

![image-20211118213152035](img/image-20211118213152035.png)



2. **《欺骗法》**前端使用JSONP方式实现跨域调用，利用<script>标签不受同源策略的限制 ①使用<script>标签让浏览器认为需要获取js，然后就会访问服务器 ②返回需要的数据并转为json，拼成一个show({...})的字符串返回给浏览器 ③浏览器会认为这是一个函数，通过提前写好的方法调用这个函数，得到需要的数据。弊端：这个请求只能是get 不能说post，而且很麻烦，适用范围太小

![image-20211118214701354](img/image-20211118214701354.png)



3. 《真正后端支持的方式》在方法上添加注解`@CrossOrigin`，并设定允许进行跨域请求的地址，即可![image-20211118220011917](img/image-20211118220011917.png)



## 1.4 ==cookie & session==

https://blog.csdn.net/chen13333336677/article/details/100939030

Cookie通过在客户端记录信息确定用户身份（不安全，局限性很大），Session通过在服务器端记录信息确定用户身份。

**Cookie的工作原理**
（1）浏览器端第一次发送请求到服务器端
（2）服务器端创建Cookie，该Cookie中包含用户的信息，然后将该Cookie发送到浏览器端
（3）浏览器端再次访问服务器端时会携带服务器端创建的Cookie
（4）服务器端通过Cookie中携带的数据区分不同的用户
<img src="img/image-20211118221341168.png" alt="image-20211118221341168" style="zoom: 50%;" />



**session的工作原理**  （session依赖于cookie）                                                                                                                                                                              （1）浏览器端第一次发送请求到服务器端，服务器端创建一个Session（不管是否使用，都会自动创建），同时会创建一个特殊的Cookie（name为JSESSIONID的固定值，value为session对象的ID），然后将该Cookie发送至浏览器端
（2）浏览器端再次访问服务器端,浏览器端访问服务器端时就会携带该name为JSESSIONID的Cookie对象
（3）服务器端根据name为JSESSIONID的Cookie的value(sessionId),去查询Session对象，从而区分不同用户。
name为JSESSIONID的Cookie不存在（关闭或更换浏览器），返回1中重新去创建Session与特殊的Cookie
name为JSESSIONID的Cookie存在，根据value中的SessionId去寻找session对象
value为SessionId不存在**（Session对象默认存活30分钟）**，返回1中重新去创建Session与特殊的Cookie
value为SessionId存在，返回session对象<img src="img/image-20211122171806026.png" alt="image-20211122171806026" style="zoom: 67%;" />

**区别**                                                                                                                                                                                                          (1)cookie数据存放在客户的浏览器上，session数据放在服务器上
(2)cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗,如果主要考虑到安全应当使用session
(3)session会在一定时间内保存在服务器上。当访问增多，会比较占用服务器的性能，如果主要考虑到减轻服务器性能方面，应当使用COOKIE
(4)单个cookie在客户端的限制是3K，就是说一个站点在客户端存放的COOKIE不能3K，且只能存放字符串，有局限性
(5)所以：将登陆信息等重要信息存放为SESSION;其他信息如果需要保留，可以放在COOKIE中



## 1.5 ==状态管理==

![image-20211122192522692](img/image-20211122192522692.png)



## 1.6 ==单点登录==

**在多个应用系统中，只需要登录一次，就可以访问其他相互信任的应用系统。**（比如进入京东首页登录，再从首页进入秒杀专区，这两个域名不同，但是只需要登陆一次）

**解决方法：**

分为两种

**①根域名相同**，关键点是设置cookie的有效范围![image-20211122194007156](img/image-20211122194007156.png)

**②根域名不同**，要利用第三方服务器颁发全局token作为全局登录凭证![image-20211122201222180](img/image-20211122201222180.png)

1. 用户访问系统1的受保护资源，系统1发现用户未登录，跳转至sso认证中心，并将自己的地址作为参数
2. sso认证中心发现用户未登录，将用户引导至登录页面
3. 用户输入用户名密码提交登录申请
4. sso认证中心校验用户信息，创建用户与sso认证中心之间的会话，称为全局会话，同时创建授权令牌
5. sso认证中心带着令牌跳转会最初的请求地址（系统1）
6. 系统1拿到令牌，去sso认证中心校验令牌是否有效
7. sso认证中心校验令牌，返回有效，注册系统1
8. 系统1使用该令牌创建与用户的会话，称为局部会话，返回受保护资源
9. 用户访问系统2的受保护资源
10. 系统2发现用户未登录，跳转至sso认证中心，并将自己的地址作为参数
11. sso认证中心发现用户已登录（这里是基于浏览器的cookie），跳转回系统2的地址，并附上令牌
12. 系统2拿到令牌，去sso认证中心校验令牌是否有效
13. sso认证中心校验令牌，返回有效，注册系统2
14. 系统2使用该令牌创建与用户的局部会话，返回受保护资源

**SSO系统登录后，跳回原业务系统时，带了个参数ST（登录凭证），业务系统还要拿ST再次访问SSO进行验证，觉得这个步骤有点多余。他想SSO登录认证通过后，通过回调地址将用户信息返回给原业务系统，原业务系统直接设置登录状态，这样流程简单，也完成了登录，不是很好吗？**

**其实这样问题时很严重的，如果我在SSO没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，是不是业务系统也认为登录了呢？这是很可怕的。**

怎么做到全局token失效的时候局部token也失效？

当全局token失效的时候，sso会通过webservice通知子系统把局部token也销毁



# 2. 商品列表与详情

![image-20211122204803366](img/image-20211122204803366.png)

**==把商品详情和库存存成两张表==，如果不拆，在一张表里，下单减库存的时候，一定要锁定这个商品的整行，在高并发的场景下，影响其他人的读，拆开之后，锁的只是库存表的一行，不影响商品详情的读**

## 2.1 ==代码实现==

**定义实例类**：

- Item商品，除了id name等属性，还加入了ItemStock和Promotion两个对象属性，并且也加了@NotNull和@NotBlank
- ItemStock商品库存，加了@Min 限制商品库存最小值为0
- Promotion 活动

**dao层**：

- ItemMapper 增删改查等方法，其中有一个查询活动商品，ItemMapper.xml写SQL语句，查询活动商品就是查询当前时间在商品活动范围内的商品
- ItemStockMapper 定义了根据ID查库存数据，ItemStockMapper.xml写SQL语句
- PromotionMapper 定义了根据商品查活动，PromotionMapper.xml写SQL语句

**service层**

- ItemService, 
  - `List<Item> findItemsOnPromotion()`查询出所有正在活动的商品,具体实现，通过遍历正在活动的商品，每次遍历的时候，分别再查一下该商品的库存和活动，一起存入List<Item>；
  - `Item findItemById(int id)`,根据id查商品，然后再查库存和活动塞进去返回
  - `decreaseStock()`减库存 `increaseSales`加库存

**controller层**(首先使用`@CrossOrigin`实现跨域)

- `getItemList()`返回商品数据集合
- `getItemDetail` 通过id返回商品详情



## 2.2 ==数据库索引==

`explain + 语句` 返回执行计划；通过type看出来这是主键扫描；possible_key可以用的索引；key用到的索引；key_len用到的索引的长度；filtered覆盖率；

![image-20211122210349842](img/image-20211122210349842.png)

这部分还是看MySQL的总结部分吧



## 2.3 ==慢查询分析==

慢查询日志，顾名思义，就是查询慢的日志，是指mysql记录所有执行超过long_query_time参数设定的时间阈值的SQL语句的日志。该日志能为SQL语句的优化带来很好的帮助。默认情况下，慢查询日志是关闭的，要使用慢查询日志功能，首先要开启慢查询日志功能。

**慢查询日志，顾名思义，就是查询慢的日志，是指mysql记录所有执行超过long_query_time参数设定的时间阈值的SQL语句的日志。该日志能为SQL语句的优化带来很好的帮助。默认情况下，慢查询日志是关闭的，要使用慢查询日志功能，首先要开启慢查询日志功能。**



# 3. 用户下单与秒杀

![image-20211122214330342](img/image-20211122214330342.png)

## 超买与少卖

下单的时刻先判定该商品有没有活动，有活动就走秒杀价（秒杀操作），没有活动就走日常价

在下单时减库存（存在少卖问题）和付款时减库存（存在卖超问题）有很大差别

**关键点1：**（主流方式）下单减库存，比如有100个人下单，下单的时候锁库存，一定不会有卖超的问题，但是可能卖少了，最后80个人付款，剩下20件商品回滚库存，**我们通过下单的时候锁库存，从业务层面解决了超卖问题**。项目里是**先在缓存中完成计数，然后再通过消息队列异步地入库**。redis由于其高速+单进程模型，省掉了很多并发的问题，所以可以被选来进行高速秒杀的工作。先利用redis的原子操作在减库存，如果库存为负，再把库存加回去，然后打上售罄标识。还有一种办法，用watch乐观锁监视

（体验不好，一般不这么干）付款减库存，100个人付款，可能有120个人下单，这就有卖超的问题

**关键点2：**order_info订单表里主键是varchar类型的很长一串，前八位是时间，后面是一个递增的流水号，不设置成自增的int是因为**方便后期拆分表**，比如把半年前的订单拆分出来，如果是自增的id就没法区分哪些是半年前的，得去查时间，这就慢了

**关键单3：**serial_number记录了当前最大的订单流水号和步长，这是为了生成新订单的流水号。此外，每次查询最大流水号的时候都加一个排它锁，防止被其他事物影响（要不然有可能俩人订单号重复了）



解决少卖问题：用延时队列解决--在下单这一刻，把处理加到延时队列里，过半个小时再消费一下，如果已经付款就移出队列，如果没有付款就把付款消息作废并移出队列。延时队列有以下两种实现

**redis**![image-20211208220507342](img/image-20211208220507342.png)

rocketMQ天生就支持延时队列![image-20211208220724809](img/image-20211208220724809.png)



## 3.1 ==代码实现==

**dao层：**

- 自动生成的增删改查就够了
- **在`SerialNumberMapper.xml`中的selectByPrimaryKey加了一个`for update`**，for update是一种行级锁，又叫排它锁。这里是查询最大的订单号，因为每次查最大订单后之后就要进行修改，所以为了防止出现并发的问题，加了一个排它锁，
  - **具体来说，如果不加这个X锁，比如线程一在改订单数据，线程二来读，这时候读到的就是MVCC里的历史版本数据，就会出现问题，这里加一个X锁，读和写就没法同时进行了，即强制不让读历史数据**

**service层：**

- `ItemServiceImpl`的扣减库存方法，返回boolean ，如果rows>0 说明扣减库存成功，如果rows<0,说明扣除失败（因为超过最大库存了,在dao层加了一个扣减库存量必须小于库存总量），**这样的话就不用先查询库存量 再进行比较 再进行扣减 提高效率**
- `OrderService`定义`createOrder(int userId, int itemId, int amount, Integer promotionId)`创建订单，先校验参数是否合法（这里也校验库存），然后先扣库存（先把库存锁住再生成订单），再生成订单，然后更新销量（这个放最后无所谓，因为销量不影响下单，只影响观察），最后返回这个订单。

**controller层**： 

- `OrderController`从session中得到user，然后传入相应参数调用service

**再强调一下，在Spring中想要解决事物的问题，只需要在方法之上（一般就是有DML语句的方法）加一个`@Transactional`注解，就会把这个方法包围在事务范围之内，spring外层会在方法最开始begin，方法结束时commit，抛异常时回滚。怎么做到的？通过AOP做的，在开始时加一点逻辑，结束时加一点逻辑，抛异常时加一点逻辑就可以实现了**



## 3.2 ==数据库事务和锁==

看mysql的总结



# 4. 项目部署与压测

![image-20211125202646458](img/image-20211125202646458.png)

**压测的逻辑：在尽可能跑满服务器指标的情况下，尽可能提高并发量。**



**部署架构**

![image-20211125204705999](img/image-20211125204705999.png)



- 把项目打包成jar包，上传到服务器上，安装jdk1.8

- 用后台的方式启动jar，并且需要指定一下jvm（默认的jvm内存空间比较小，修改大一点），以脚本的方式启动jar![image-20211125212510131](img/image-20211125212510131.png)

  ​	nohup 控制台如果要产生信息，把他记录在一个文件里，不要打印在控制台，后面&结尾，表示后台启动，这个窗口关了应用也不会挂掉，`-Xms1024m -Xmx1024m -XX:NewSize=512m -XX:MaxNewSize=512m`,最小内存和最大内存都是1G，防止内存大小伸缩影响性能，新生代512m，最大新生代也是512m

  `pid=`ps aux | grep seckill | grep -v grep | awk '{print $2}' `
  kill -9 $pid`这两句话是查一下seckill的端口，查到之后kill掉

- 然后安装Nginx，并配置web服务和反向代理

- 本地安装jmeter并配置，先测详情页面，压测的时候不断增大线程数，在不出错的情况下找到最大值，此时主要瓶颈是带宽，总共测试300个线程，10秒内起来，一共跑10次，吞吐量224吞吐量只有300

![image-20211129195249140](img/image-20211129195249140.png)

- 再测下单操作，下单需要登录，在jmeter中，我们先传入参数，再把登录后的cookie传入就可以了，总共测试300个线程，10秒内起来，一共跑10次，吞吐量224

![image-20211129202359484](img/image-20211129202359484.png)



## 后期优化方向

1. Nginx主从热备
2. Tomcat做集群
3. 两级缓存
4. MySQL读写分离
5. 用MQ做异步，提高性能

![image-20211129204740432](img/image-20211129204740432.png)



### MySQL读写分离

读写分离建立在主从服务器基础上，主从服务器是基于bin log 和 relay log实现的

![image-20211129210302787](img/image-20211129210302787.png)



### MySQL分布式事务

分布式事务顾名思义就是要在分布式系统中实现事务，它其实是由多个本地事务组合而成。

一般尽可能避免做分布式事务，如果一定要做（比如跨行转账这个事务），解决方案 2PC两阶段提交

1. 开始事务时，事务管理器向所有的数据库发送一个开始的命令，
2. 然后数据库执行应用程序给的SQL
3. 执行完了之后（此时还没提交），事务管理器发送prepared指令，看返回结果是不是所有数据库都执行了
4. 如果都没问题就发布提交指令，只要有一个有问题就回滚指令

![image-20211129211624072](img/image-20211129211624072.png)

这个机制不是100%成功，因为事务管理器有可能挂了或者命令没传过去，所以加一个重试机制查看是否正常



### MySQL内部分布式事务机制

解决的是bin log 和 redo log不同步的问题，详见MySQL总结



# 分布式状态管理

开始v2

![image-20211129214317492](img/image-20211129214317492.png)

在服务器上安装redis，更改配置文件

**redis：key是字符串，value可以使是五种**

<img src="img/image-20211129220034893.png" alt="image-20211129220034893" style="zoom:67%;" />



## 1. ==redis安装与使用==

- 导入redis的包，并配置等，此外，写一个redis配置类覆盖原有的配置类`RedisConfiguration`,改掉序列化的方式，在序列化前加上类名，反序列化的时候就可以直接转换为我们需要的类，而不是object
- redis支持简单的事务，![image-20211130194555833](img/image-20211130194555833.png)



## 2. ==代码实现==

要用redis代替session做状态管理，我们用到session的地方有**UserControlle**，**OrderController**，**LoginCheckInterceptor**

1. **UserControlle**

   - 注册原来把验证码存入session中，现在把这个验证码存入redis中，并且设置5分钟的有效时间。![image-20211130200152624](img/image-20211130200152624.png)

     从redis中获取验证码![image-20211130200249439](img/image-20211130200249439.png)

   - 登录，原先把user存入session中，记录了登录状态，下次可以直接看能不能取到loginUser判断是否登录。现在存到redis里，随机生成一个字符串记为token作为key，value是user，有效时间一天![image-20211130200719507](img/image-20211130200719507.png)

   - 登出的时候传入token，在redis里删除这个token就可以了

2. **OrderController**，在创建订单的时候需要记录userId，原先是从session里取user

   - 现在在方法里传入token，通过token在redis里取user就可以了![image-20211130201130347](img/image-20211130201130347.png)

3. **LoginCheckInterceptor**拦截器，在指定的请求前面做拦截，判断是否登录，原先是看能不能从session里取到user

   - 现在用redis，首先从request中获取token，然后再去redis中查user![image-20211130202132038](img/image-20211130202132038.png)



# 缓存商品与用户

![image-20211201162918365](img/image-20211201162918365.png)

**用redis做缓存，提升查询时的性能；不仅用到了redis，还用到了guava（瓜哇），用这两个做了个两级缓存。为什么不用MySQL或MyBatis的二级缓存，因为这俩离数据源太近，离用户比较远，我们要尽可能让缓存靠前。 我们用guava做一级缓存，guava是和tomat在一起，是本地缓存，最快，但不能占太大空间，只有特别热的数据才放到一级缓存，其余的放在二级缓存（redis）。**

## ==1. 代码实现==

改缓存对于前端来说没有任何变化，该怎么请求还是怎么请求，只是在查找的时候，把命中的数据放在缓存里，期待下次对数据的访问，所以都是在service层

1. **ItemServiceImpl**,注入redis（远程缓存，也就是二级缓存）和cache（本地缓存，也就是一级缓存）

   初始化瓜哇缓存，初始化容量10，最大100，过期时间1分钟

   - 原先有一个查商品详情的方法`findItemById`是从MySQL中查，现在我们加一个`findItemInCache`,从缓存里查，（）先去瓜哇查，再去redis 查，再去mysql查，在更多的时候可以替代`findItemById`

   ![image-20211201160011211](img/image-20211201160011211.png)

   - 用这个格式，key是`"item:" + id`，item是商品详情，在瓜哇和redis里通过key找item。①第一次找的时候，瓜哇和redis都没有，去，mysql里找到，存入瓜哇和redis；②第二次找的时候，直接在瓜哇里就能找到，返回该数据；③瓜哇有效时间是1分钟，1分钟后瓜哇失效，此时在redis中可以找到，然后再存入瓜哇

   

2. **UserServiceImpl**，因为在登录之后，很多地方都需要请求用户的信息(此 项目是在orderController里用)，而且因为用户信息更改的频率很低，所以可以把用户也存入缓存里

   - 原先如果要查用户信息，是有一个`findUserById`,现在写了一个`findUserFromCache`来替换这个方法；这个方法只引入了redis，没有引入瓜哇，也就是只用了远程缓存，不用本地缓存，因为本地缓存只存入最热的数据，在线用户数据量太大，而且不一定所有用户都是活跃状态，所以只用redis
   - 逻辑和商品缓存一样，先看redis里有没有，没有的话去mysql查，查到之后存入redis（30分钟有效期）

3. controller不调用`findItemById`和`findUserById`，调用`findItemInCache`和`findUserFromCache`



## ==2. 缓存的淘汰策略==

### 1. 数据过期策略（被动淘汰）

![image-20211201163408251](img/image-20211201163408251.png)

### 2. 内存淘汰策略（主动淘汰）

![image-20211201163606138](img/image-20211201163606138.png)

![image-20211201163821128](img/image-20211201163821128.png)



## ==3. 缓存与数据库的同步==

![image-20211201164137257](img/image-20211201164137257.png)

![image-20211201192622406](img/image-20211201192622406.png)

那为什么要先更新数据库，再删除缓存呢？ 

如图所示，都是基于第二步会失败的情景：

上半部分，先删缓存，再更新数据库，再更新数据库如果失败了，B线程此时来查，在缓存里已经查不到了，就去数据库查，查到之后再存到缓存里，此时重试更新数据库，如果成功了，可是此时数据库与缓存又不一致了

下半部分，就不会出现这种情况，但是会出现一些线程得到旧数据的情况

![image-20211201193242312](img/image-20211201193242312.png)

如图所示，是基于第二步会成功的情景：

1. 上半部分，先删缓存，再更新数据库，删掉缓存之后，B线程来读，缓存里读不到，去数据库查，然后set到缓存里，然后在更新数据库，此时数据库和缓存又不同步了
2. 下半部分，先更新数据库，然后B来读，从缓存里读到了旧数据，然后再删缓存，此时同步了，只是有一些线程会读到旧数据

![image-20211201193833111](img/image-20211201193833111.png)

综上，无论第二步是否会失败，先删缓存总是会出现一些不同步的情景，而先更新数据库，只会有一点小问题（一些线程会读到旧数据）



## ==4. 分布式缓存常见问题 缓存穿透、缓存击穿、缓存雪崩==

### 1. 缓存穿透

![image-20211201194722916](img/image-20211201194722916.png)

### 2. 缓存击穿

![image-20211201194933947](img/image-20211201194933947.png)

### 3. 缓存雪崩

![image-20211201195205561](img/image-20211201195205561.png)



# 异步化扣减库存

![image-20211201210105015](img/image-20211201210105015.png)

消息队列的作用：**异步、解耦、削峰。** 

## RocketMQ

RocketMQ主要由 Producer、Broker、Consumer 三部分组成，其中Producer 负责生产消息，Consumer 负责消费消息，Broker (消息队列服务器)负责存储消息。Broker 在实际部署过程中对应一台服务器，每个 Broker 可以存储多个Topic的消息，每个Topic的消息也可以分片存储于不同的 Broker。Message Queue 用于存储消息的物理地址，每个Topic中的消息地址存储于多个 Message Queue 中。ConsumerGroup 由多个Consumer 实例构成。

**部署架构**

![image-20211201213532980](img/image-20211201213532980.png)



## ==代码实现==

两阶段提交



先在服务器上安装rocketmq，本地代码导包即可

### 增加销量

**增加销量和下单不用保持事务性，因为销量不影响购买**

**service层：**

- 原先是在`OrderServiceImpl`中生成订单后增加销量，增加销量也是锁住Item的一行，会影响并发性能，现在
  1. 向Server发送第一阶段的消息，主题是`seckill`标签是`increase_sales`，消息内容包括商品ID和数量，成功失败都记录日志，超时时间设置60s
  2. `IncreaseSalesConsumer`中定义消费主题是`seckill`标签是`increase_sales`的消费者，消费者从消息队列中取出消息并消费（调用`itemService.increaseSales`方法）



### **扣减库存**

![image-20211206202254554](img/image-20211206202254554.png)

这个必须是事务性操作。这里的事务性是指在缓存里扣库存和数据库扣库存必须是事务性的。首先需要利用`cacheItemStock`预热缓存（因为下单扣减库存先在缓存里减，需要活动开始前先提前把库存都导入到缓存中,具体操作是把所有商品查出来，针对每个商品把库存存入redis中）。

事务性的异步扣减库存关键在于订单流水，在数据库里有一个item_stock_log流水表，id是随机字符串（因为订单之间无关），item_id,amount,status(表示状态，0表示不知道，1代表成功，2代表失败)

**service层：**

- 先在`ItemServiceImpl`写一个在缓存里扣减库存的方法`decreaseStockInCache`,如果减完库存后值>=0，说明扣减成功，反之失败。
- `LocalTransactionListenerImpl`类中定义执行本地事务的方法，先判断来的消息的tag是不是`decrease_stock`，如果是，就执行`createOrder`（创建订单，这里给创建订单的方法加了个更新流水），
- `LocalTransactionListenerImpl`类中定义回查的方法，先判断来的消息的tag是不是`decrease_stock`，如果是，就执行`checkStockStatus`（通过传入的流水ID去查流水的status）

- 原先是在`OrderServiceImpl`中创建订单时扣减库存，现在createOrder方法： ①调用`decreaseStockInCache`先扣减缓存里的库存，然后通过返回值判断是否成功（如果失败就报库存不足）②生成订单 ③更新流水状态为1
- `DecreaseStockConsumer`中定义消费者，在数据库中扣减库存
- 定义`createOrderAsync`方法,先判断该商品是否具有售罄标识（如果已经售罄直接返回），如果没有售罄，①生成订单流水，②发消息，消息体里存了商品ID 下单数量 订单流水ID（回查的时候需要用到），再定义一个本地事务需要的参数 ③投送消息

controller层

- 原本在creat中调用orderService.createOrder创建订单并扣减库存，现在调用orderService.createOrderAsync



## 消息丢失问题

RocketMQ支持消息的高可靠，影响消息可靠性的几种情况：

1. Broker非正常关闭
2. Broker异常Crash
3. OS Crash
4. 机器掉电，但是能立即恢复供电情况
5. 机器无法开机（可能是cpu、主板、内存等关键设备损坏）
6. 磁盘设备损坏

1)、2)、3)、4) 四种情况都属于硬件资源可立即恢复情况，RocketMQ在这四种情况下能保证消息不丢（消息存在硬盘上），或者丢失少量数据（依赖刷盘方式是同步还是异步）。

5)、6)属于单点故障，且无法恢复，一旦发生，在此单点上的消息全部丢失。RocketMQ在这两种情况下，通过异步复制（主从复制），可保证99%的消息不丢，但是仍然会有极少量的消息可能丢失。通过同步双写技术（相当于都是主节点）可以完全避免单点，同步双写势必会影响性能，适合对消息可靠性要求极高的场合，例如与Money相关的应用。注：RocketMQ从3.0版本开始支持同步双写。

总结 可以不丢失消息 但是性能会有损失，如果可以容忍丢失极少量消息，性能会有很大提高



## 消费失败问题

### 消息重试

Consumer消费消息失败后，要提供一种重试机制，令消息再消费一次。Consumer消费消息失败通常可以认为有以下几种情况：

- 由于消息本身的原因，例如反序列化失败，消息数据本身无法处理（例如话费充值，当前消息的手机号被注销，无法充值）等。这种错误通常需要跳过这条消息，再消费其它消息，而这条失败的消息即使立刻重试消费，99%也不成功，所以最好提供一种定时重试机制，即过10秒后再重试。
- 由于依赖的下游应用服务不可用，例如db连接不可用，外系统网络不可达等。遇到这种错误，即使跳过当前失败的消息，消费其他消息同样也会报错。这种情况建议应用sleep 30s，再消费下一条消息，这样可以减轻Broker重试消息的压力。

### 消息重投

消息重投是为了解决消息投送失败的问题，但是可能会引起重复消费的问题，因为比如这个消息已经投送成功，但是发个生产者的ack丢失，生产者会主动重发

 解决方法：**幂等** 。在编程中一个*幂等* 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。

### 死信队列

死信队列用于处理无法被正常消费的消息。当一条消息初次消费失败，消息队列会自动进行消息重试；达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列 不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中进行特殊处理



# 削峰限流与防刷

![image-20211207204928553](img/image-20211207204928553.png)

## 削峰限流

**削峰限流解决的是下单这个借口流量大的问题（比如一次进来100万的流量，服务器肯定崩溃了）**

解决方案如下

![image-20211207211011032](img/image-20211207211011032.png)

## 代码

controller层

- **验证码**：`getCaptcha`得到验证码，验证token，然后把验证码存入redis（user，验证码文本），1分钟有效期
- **请求令牌**：`generateToken`先验证验证码是否正确，然后调用`promotionService.generateToken`（这个方法检测了售罄标识、校验用户、校验商品活动等，然后判断秒杀大闸到没到限制，就是减去redis的设定值，我设定的是库存的5倍，如果大闸到0，就不颁发令牌，否则就返回一个有效期为10分钟的令牌）生成令牌，判断令牌是否为空，如果不为空，就到创建订单
- **限流器**：`create`创建订单里面，首先限制单机流量（申请访问，1秒内允许就可以，否则就是系统繁忙），然后再验证上一步得到的令牌是否正确，
- **队列缓冲**：然后利用线程池（线程池提前做了配置）执行，就起到了队列缓冲的作用，利用多线程下单



### 限流器

限流器有两种算法

**令牌桶算法（业务有可能出现爆发的情况，比如一开始没有请求，突然爆发来了100个，业务组件就要同时处理100个）：**限流器初始化，同时初始化令牌桶，假设令牌桶容量100，一开始初始化给桶里面放10个令牌，然后令牌生成器每秒添加10个令牌，桶满则弃掉多余令牌。当客户端访问时，会被限流器拦截，然后看有没有令牌，有的话桶里令牌-1并允许访问业务组件，没有则直接返回![image-20211207220840558](img/image-20211207220840558.png)

**漏桶算法（业务组件处理速度恒定，很稳健，永远只处理10个请求）：**客户端访问时（视为一滴水），被限流器拦截，尝试将这滴水加给漏铜，如果此时桶已经满了，直接返回，如果桶未满，则传入漏铜假设漏铜容量100，水满则溢，每秒漏出10滴水，漏给业务组件![image-20211207221451196](img/image-20211207221451196.png)

如果要求服务器能够很稳健的处理请求，用漏桶算法，如果希望能偶尔处理大规模突发请求，但是不能长期，用令牌桶算法，在我们的系统里，在秒杀那一刻会涌入大量请求，所以我用令牌桶算法。



## 防刷

削峰是削的正常的用户，防刷是防止黄牛

前两种比较老 第三种比较好，但是每种方法都有破解的方法，往往结合多种方法配合大数据和AI一起解决

<img src="img/image-20211207220112975.png" alt="image-20211207220112975" style="zoom: 50%;" />



# 最后压测

购买按需付费的服务器，原先的服务器因为带宽受限，现在买一个带宽上限是300Mbit/s的服务器再次压测。

商品详情 2000个线程 20s跑完 跑20轮  吞吐量大概是5500

或者是就在自己的服务器上压测 



后续优化方向：该做的都差不多了，剩下就是可以把功能补充完整，例如 

①付款环节 、

②解决少卖的问题（用延时队列，超时取消订单）、

③整个系统都是单节点部署，可以把tomcat mysql redis nginx mq都做成集群的方式 

再有就是一些细节，例如

④线程池的数量多少比较合适（需要不断测试去调整参数）

